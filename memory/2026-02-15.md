# 2026-02-15 — Blofin Strategy Evolution Overhaul

## Summary
Redesigned Blofin strategy system from manual tuning to fully automated AI-driven evolution. Removed kanban references (legacy). Built comprehensive plan for continuous strategy generation + ML model evolution pipeline.

## Work Done

### Cleanup
- Removed all kanban references from README.md

### AI Review Execution
- Disabled 6 underperforming strategies (bb_squeeze, breakout, candle_patterns, momentum, reversal, support_resistance)
- Tuned 2 borderline performers (rsi_divergence, vwap_reversion) via Sonnet subagent
- Left 5 active strategies (ema_crossover, macd_divergence, rsi_divergence, volume_spike, vwap_reversion)

## Complete System Design: Unified AI Trading Pipeline v3

### Core Architecture
- **Backtest-first approach:** 7-day historical data (1min, 5min, 60min)
- **20 active strategies** at all times (auto-designed, tuned, replaced if stalled)
- **5 active ML models** (trained every 12h on rolling 7-day windows)
- **Ensemble combinations** (tested, ranked, composable)
- **Daily orchestration** (12h cycle: score, design/tune, validate, rank, report)

### Three-Stream Parallel System

**Stream 1: Strategy Evolution (48h cycle)**
```
Design (Opus, 15 min)
→ Backtest (Sonnet, 20 min)
→ Validate vs live (Haiku, 5 min)
→ Rank & update pool (keep top 20)
```

**Stream 2: ML Pipeline (12h cycle)**
```
Build 5 models (Sonnet, 30 min parallel)
→ Backtest each (auto, 20 min)
→ Test ensembles (Sonnet, 15 min)
→ Rank & deploy (keep top 5)
```

**Stream 3: Shared Knowledge Base**
- Market regime detection
- Failure pattern analysis
- Feature importance tracking
- Design history + lessons learned

### Key Features

1. **Feature Library** (single source of truth)
   - 50+ features: price, volume, technical, volatility, regime
   - Unified API: `get_features(symbol, timeframe, feature_list)`
   - Strategies + ML tap same features (no reimplementation)

2. **Backtester** (7-day replays)
   - Strategy backtester (execute detect() on historical data)
   - Model backtester (predict vs actual, calc accuracy/F1)
   - Multi-timeframe testing (1m, 5m, 60m)
   - Metrics: score, Sharpe, win_rate, max_drawdown, accuracy, precision, F1

3. **ML Models** (separate folders, composable)
   - Direction predictor (XGBoost) → UP/DOWN
   - Risk scorer (Random Forest) → 0-100 risk level
   - Price predictor (Neural Net) → future price
   - Momentum classifier (SVM) → acceleration/deceleration
   - Volatility regressor (Gradient Boosting) → future volatility

4. **No Hard Thresholds**
   - Always keep top 20 strategies
   - Always keep top 5 models
   - Dynamic ranking, not pass/fail
   - Better performers keep slots longer

5. **Daily Reporting**
   - Human-readable: "Designed X, tuned Y, replaced Z"
   - AI-readable JSON: strategy/model changes, performance trends
   - Next-cycle recommendations from Opus

### Implementation Plan (6 Weeks)

**Phase 1: Foundation (Week 1)**
- Build feature_manager.py (50+ features)
- Build backtester + aggregator
- Update database schema

**Phase 2: ML Pipeline (Week 2)**
- Model framework (base class + trainers)
- Train 5 model types
- Validate + tune
- Ensemble predictor

**Phase 3: Strategy Evolution (Week 2-3)**
- Update strategies for backtest mode
- Strategy tuner (Sonnet designs params)
- Strategy designer (Opus creates new strategies)
- Strategy ranker

**Phase 4: Orchestration (Week 3)**
- Daily runner (12h cycles)
- Reporter (human + AI)
- Cron setup

**Phase 5: Testing & Docs (Week 3-4)**
- Unit + integration tests
- Performance benchmarking
- Documentation

**Phase 6: Deployment (Week 4+)**
- Pre-launch checklist
- Gradual go-live (backtest → design → ML → full automation)
- Monitoring dashboard

### Costs

**Development:** ~$900 (mostly Opus for strategy design)
**Daily operation:** ~$2.20/day (~$66/month)
- Backtesting, ML training, design cycles
- Mostly Sonnet + some Opus

### Documentation Created

- ✅ ARCHITECTURE.md (15,696 bytes) - Complete system design
- ✅ FEATURE_LIBRARY.md (10,708 bytes) - All 50+ features, how to add
- ✅ BACKTEST_GUIDE.md (12,938 bytes) - Backtesting strategies + models
- ✅ ML_PIPELINE.md (13,641 bytes) - Training, validation, tuning
- ✅ ENSEMBLE_GUIDE.md (14,564 bytes) - Combining models
- ✅ IMPLEMENTATION_PLAN.md (15,313 bytes) - Phased build plan + timeline

## Open Design Questions (Resolved)

1. ✅ Backtest window: 7 days (last week of data)
2. ✅ Aggregations: 1min, 5min, 60min
3. ✅ Live validation: 7 days running (then move to real money later)
4. ✅ ML features: Separate library for any feature on demand
5. ✅ Reporting: Daily human-readable + AI-readable JSON
6. ✅ Thresholds: Dynamic ranking (keep top X, no hard pass/fail)
7. ✅ Model organization: Separate folders, composable ensembles
8. ✅ Mode: Backtest-only initially (faster iteration than live waiting)

## Next Steps

1. **Review plan with Rob** (ask questions, get approval)
2. **Spawn Sonnet subagent** to start Phase 1 (feature library)
3. **Daily progress tracking** (what was built, what's next)
4. **Iterative integration** (build, test, integrate, move to next component)

## Key Insight

This is no longer a "strategy portfolio" system. It's now an **AI research platform for discovering trading strategies and building ML models**. The system itself generates candidates, tests them rigorously (backtest-first), and keeps only what works. Zero manual design. Pure AI-driven evolution.

With 50+ features, 20 strategy slots, 5 ML slots, and ensemble combinations, the search space is massive. Backtest-first means we fail fast and pivot quickly.

**Expected outcome:** In 4 weeks, should have a proven library of strategies + models ready for small live testing.
