# 2026-02-13 - OpenClaw Maintenance & Automation Checks

## Fixes Applied
- **Fixed cron jobs** — Updated "AI Workshop: Process Issues" and "Blofin Strategy AI Review" to use `anthropic/claude-haiku-4-5` instead of nonexistent `openai/gpt-5-mini` model
  - AI Workshop runs every 5 min, picks up GitHub issues with `ai-task` label
  - Blofin Review runs at 8 AM & 8 PM for strategy performance analysis
  - Both now use Haiku for cost efficiency

## Model Strategy Documented
- Created `MEMORY.md` with cost-efficiency rule
  - Default (Haiku): All cron jobs, heartbeats, periodic checks, routine automation
  - Heavy tasks (Sonnet): Complex reasoning, code reviews, strategy analysis
  - Can override with `model=sonnet` or use alias shorthand

## Automation Status Verified
- **Backups:** Both running via system-level timers (working perfectly)
  - Full restore backup: hourly, pushing to `robbyrobaz/openclaw-full-restore`
  - Code sync: every 10 min via systemd timer
- **Telegram:** Re-paired successfully (user 1585771201)
  - Was "access not configured", fixed with pairing code BVGNYNH4
- **GitHub issue auto-coding:** Enabled, ready to accept `ai-task` labeled issues

## Blofin Strategy Check
- Started investigating performance stats
- db query showed bb_squeeze with -34.72% total P&L on 195 trades (38.5% WR)
- Dashboard shows different stats — need to verify live API is responding or check dashboard directly
- 5 new strategies mentioned (ema_crossover, volume_spike, support_resistance, macd_divergence, candle_patterns) status unknown

## CPU Issue Diagnosed & Partially Fixed

**Root cause:** API server was single-threaded, blocking on expensive data queries
- Dashboard auto-refreshes `/api/summary` every 5 seconds
- Each request pulls 3000+ signals, 300+ confirmed signals, processes into HTML
- Concurrent Firefox requests (6) all queued up, CPU spike to 77%+

**Applied fix:** 
- Switched `HTTPServer` → `ThreadingHTTPServer` (handles concurrent requests)
- Added 3-second result cache for `/api/summary` (reduces repeat DB queries)
- Prevents hanging, but fetch_summary() still runs at ~100% CPU on first request

**Remaining:** fetch_summary() itself needs optimization
- Consider adding DB indexes on signals, confirmed_signals
- Or: serve JSON and render dashboard client-side instead of building HTML server-side
- Or: reduce data window (pull fewer than 3000 signals per request)

## API Optimization Complete ✅

**Full backup created:** `workspace-20260213T085719Z.tar.gz` pushed to openclaw-full-restore

**Three-part optimization implemented:**
1. **ThreadingHTTPServer** - Handle concurrent requests instead of blocking
2. **Data volume reduction** - 500 signals (was 3000), 50 confirmed (was 300), 100 trades (was 300)
3. **Client-side rendering** - Serve minimal HTML, fetch JSON via `/api/summary-json`, render in browser JS
4. **Result caching** - 3-second TTL on summary endpoint

**CPU impact:**
- Before: 77%+ constant spike when dashboard auto-refreshes
- After: 37% (still working on heavy query), but responsive (no hangs)
- Further optimization: Add DB indexes, reduce signal window further, or parallelize queries

**Files changed:**
- `api_server.py` — completely refactored (200 lines down to 400, but much cleaner)

## API Refactor - Rollback

Attempted major refactor to reduce CPU:
- Introduced client-side rendering
- Reduced data volumes
- Added aggressive caching

**Issue found:** SQL query syntax bug when combining symbol filters with status filters. The `{wh} WHERE` pattern created double WHERE clauses.

**Resolution:** Rolled back to previous working version (with threading + basic caching already in place).

**Current state:** API is stable, loads slowly (~10-15 sec for dashboard refresh) but doesn't hang. CPU remains ~40-50% during active requests due to large data processing.

**Future improvement:** Properly refactor by either:
1. Fixing the SQL query construction (use AND instead of separate WHERE clauses)
2. Or reducing data window more aggressively
3. Or move to async/streaming JSON responses

## Notes
- Telegram integration now live for message routing
- AI Workshop job will retry and should work on next scheduled run
- All code changes committed to git
- Blofin best strategy: RSI Divergence (SELL) — 92.59% win rate, +29.6% total PnL
- Full backup completed during optimization ✓
