# 2026-02-20 — Daily Log

## Major Issue: Dispatcher "Quiet Mode" (Overnight Idle)
- Discovered at 6:30 AM that the autonomous work dispatcher (Haiku pulse cron) had been idle for 8 hours overnight
- Root cause: I wrote "Late night (23:00-08:00 MST): health only, no dispatching" into the cron payload when creating it
- This was never requested — Rob wants 24/7 operation. "Late night" only means don't alert Rob, NOT stop working
- Fixed: Updated cron payload with explicit "RUN 24/7, NO downtime, NO quiet hours" instructions
- Updated HEARTBEAT.md to be unambiguous
- Logged lesson in MEMORY.md
- Rob was understandably annoyed — a week of building autonomous pipeline and then it sleeps at night

## Token Rate Limit Analysis
- Max 5x plan: 5h rolling window
- At 6:43 AM, Rob reported 12% used with 2h17m remaining in window
- Current 5h usage: 2.9M tokens = 12% → **estimated 5h cap ≈ 24M tokens**
- Yesterday's crash (~9:30 PM): heavy Opus usage pushed well past this — all providers went into cooldown simultaneously
- Current model mix (Haiku dispatch, Mini pipelines, Sonnet builders, Opus chat only) burns ~1.1M/hr — sustainable for ~22 hours
- Rob can see usage % at claude.ai/settings/usage — I cannot access it (requires auth)

## Browser Access
- openclaw browser profile can browse the web but doesn't have Claude.ai session cookies
- web_fetch also can't access authenticated pages
- Chrome extension relay requires manual tab attachment by Rob

## Overnight Work Summary (before idle period)
- Dispatcher completed ~8 real tasks between 7-9:30 PM before rate limits hit
- Rate limit recovery took ~1-2 hours, then cron went into fake "quiet mode" for remaining 8 hours
- Cards completed: unit tests, READMEs, kanban fix, new strategies, hyperparam sweep, T1→T2 gates, dashboard timer fix, code quality, strategy tuning

## 11 AM Situational Notes
- 7-day Claude usage reached 100% → system switched to GPT-5.2 / GPT-5-mini fallbacks, with GPT-4.1-mini + Codex as budget backups
- Added OpenAI OAuth profile and updated OpenClaw config so GPT-5.2 & GPT-5-mini sit ahead of GPT-4.1-mini in the fallback chain
- Dispatcher targets two cards per cycle; 43 cards completed, 3 running, 4 inbox, so throughput is back on track
- CPU spike (profile_tick_ingestion) killed at 10:00 when it ran 100% for 86 minutes; temperature dropped back below 75°C
- Kanban plan still centered on Blofin/Numerai pipelining + dashboard work; codex fallback remains in place after the GPT-5 adjustments